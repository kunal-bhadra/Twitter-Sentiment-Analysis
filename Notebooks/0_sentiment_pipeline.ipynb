{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Pipeline from Hydrated JSONL to the full state DF of multiple dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- !pip install dask\n",
    "- !pip install geopy\n",
    "- !pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSONL_done = [\"C:\\Data Science\\Jupyter_Workspace\\Twitter_Sentiment\\Data\\JSONLs\\\\20200320.jsonl\",\n",
    "               \"C:\\Data Science\\Jupyter_Workspace\\Twitter_Sentiment\\Data\\JSONLs\\\\20200325.jsonl\",\n",
    "               \"C:\\Data Science\\Jupyter_Workspace\\Twitter_Sentiment\\Data\\JSONLs\\\\20200327.jsonl\",\n",
    "               \"C:\\Data Science\\Jupyter_Workspace\\Twitter_Sentiment\\Data\\JSONLs\\\\20200405.jsonl\",\n",
    "               \"C:\\Data Science\\Jupyter_Workspace\\Twitter_Sentiment\\Data\\JSONLs\\\\20200501.jsonl\",\n",
    "               \"C:\\Data Science\\Jupyter_Workspace\\Twitter_Sentiment\\Data\\JSONLs\\\\20200517.jsonl\",\n",
    "               \"C:\\Data Science\\Jupyter_Workspace\\Twitter_Sentiment\\Data\\JSONLs\\\\20200608.jsonl\",\n",
    "               \"C:\\Data Science\\Jupyter_Workspace\\Twitter_Sentiment\\Data\\JSONLs\\\\20200701.jsonl\",\n",
    "               \"C:\\Data Science\\Jupyter_Workspace\\Twitter_Sentiment\\Data\\JSONLs\\\\20200907.jsonl\",\n",
    "               \"C:\\Data Science\\Jupyter_Workspace\\Twitter_Sentiment\\Data\\JSONLs\\\\20200921.jsonl\",\n",
    "               \"C:\\Data Science\\Jupyter_Workspace\\Twitter_Sentiment\\Data\\JSONLs\\\\20200930.jsonl\"]\n",
    "\n",
    "JSON_willDo = []\n",
    "# 1,2: 210 | 3: 74 | 4: 180 | 5: 30 | 6: 120 | 7: 45 | 8: 45 | 9: 28 | 10:18 | 11: 16\n",
    "\n",
    "# Dask Error\n",
    "# \"C:\\Data Science\\Jupyter_Workspace\\Twitter_Sentiment\\Data\\JSONLs\\\\20200330.jsonl\"\n",
    "# \"C:\\Data Science\\Jupyter_Workspace\\Twitter_Sentiment\\Data\\JSONLs\\\\20200414.jsonl\"\n",
    "# \"C:\\Data Science\\Jupyter_Workspace\\Twitter_Sentiment\\Data\\JSONLs\\\\20200801.jsonl\"\n",
    "# \"C:\\Data Science\\Jupyter_Workspace\\Twitter_Sentiment\\Data\\JSONLs\\\\20200826.jsonl\"\n",
    "\n",
    "CSVpath = \"C:\\Data Science\\GitHub Projects\\Twitter-Sentiment-Analysis\\\\final_df.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_pipeline(JSONL_list, CSVpath):\n",
    "    #importing libraries\n",
    "    from geopy.geocoders import Nominatim\n",
    "    from geopy.extra.rate_limiter import RateLimiter\n",
    "    import dask.bag as db\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "    import re\n",
    "\n",
    "    final_df = pd.read_csv(CSVpath)\n",
    "    #final_df = pd.DataFrame()\n",
    "    \n",
    "    for hydratedJSONL in JSONL_list:    \n",
    "        #STEP ONE: Extract Indian tweets with location\n",
    "        # 1.loading the big JSONL into DASK for parallelization\n",
    "        db_dask = db.read_text(hydratedJSONL).map(json.loads)\n",
    "        ind_dask = db_dask.filter(lambda tweet: tweet['place'] is not None and tweet['place']['country'] == 'India')\n",
    "\n",
    "        #function to only convert relevant data into Dask DF\n",
    "        def flatten(tweet):\n",
    "            return {\n",
    "            'id': tweet['id_str'],\n",
    "            'longitude': tweet['place']['bounding_box']['coordinates'][0][0][0],\n",
    "            'latitude': tweet['place']['bounding_box']['coordinates'][0][0][1],\n",
    "            'text': tweet['full_text']\n",
    "            }\n",
    "            \n",
    "        ind_dask_df = ind_dask.map(flatten).to_dataframe()\n",
    "        ind_df = ind_dask_df.compute() #converting Dask DF to Pandas DF\n",
    "\n",
    "        # 2.extracting State through coordinates\n",
    "        def getState(longitude, latitude):\n",
    "            from random import randint\n",
    "            user_agent = 'user_me_{}'.format(randint(10000,99999))\n",
    "            geolocator = Nominatim(user_agent=user_agent)\n",
    "            reverse = RateLimiter(geolocator.reverse, min_delay_seconds=1)\n",
    "            location = reverse((str(latitude) + \",\" + str(longitude)), language='en', exactly_one=True)\n",
    "            if location is not None:\n",
    "                if 'state' in location.raw['address']:\n",
    "                    state = location.raw['address']['state']\n",
    "                    return state\n",
    "                else:\n",
    "                    return None\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "        ind_df[\"state\"] = ind_df.apply(lambda x : getState(x[\"longitude\"], x[\"latitude\"]), axis=1)\n",
    "\n",
    "        # 3.removing rows with null State values\n",
    "        df_stepOne = ind_df[~ind_df['state'].isnull()]\n",
    "\n",
    "\n",
    "        # --------------------------------------------------------\n",
    "        #STEP TWO: Getting Sentiment Scores of Tweet texts with VADER\n",
    "        # 1.preprocessing the tweet text\n",
    "        def preprocess_for_vader(tweet):\n",
    "            import re\n",
    "            cleaned_text = tweet\n",
    "            cleaned_text = re.sub(r'RT @[\\w]*:', '', cleaned_text) #removing RT handles\n",
    "            cleaned_text = re.sub(r'@[\\w]*', '', cleaned_text) #removing all @mentions\n",
    "            cleaned_text = re.sub(r'((www.[^s]+)|(https?://[^s]+))', ' ', cleaned_text) #removing URL links\n",
    "            cleaned_text = np.core.defchararray.replace(cleaned_text, \"[^a-zA-Z#]\", \" \") #removing special characters, numbers and punctuations except #\n",
    "            return cleaned_text\n",
    "\n",
    "        df_stepOne[\"vader_text\"] = df_stepOne[\"text\"].apply(lambda x: preprocess_for_vader(x))\n",
    "\n",
    "        # 2.sentiment scoring with VADER\n",
    "        analyser = SentimentIntensityAnalyzer()\n",
    "        df_stepOne[\"vader_score\"] = df_stepOne[\"vader_text\"].apply(lambda x: analyser.polarity_scores(x)[\"compound\"])\n",
    "\n",
    "        # 3.storing in new df\n",
    "        df_stepTwo = df_stepOne.copy()\n",
    "\n",
    "\n",
    "        # --------------------------------------------------------    \n",
    "        #STEP THREE: Getting average State Score\n",
    "        # 1.grouping to get the score for each state\n",
    "        df_stepThree = df_stepTwo.copy()\n",
    "        df_stepThree = df_stepThree.groupby('state', as_index=False)['vader_score'].mean()\n",
    "\n",
    "        # 2.correcting state names to match with GeoJSON Keys\n",
    "        df_stepThree[\"state\"].replace({\"Jammu and Kashmir\": \"Jammu & Kashmir\", \n",
    "                                \"Dadra and Nagar Haveli and Daman and Diu\": \"Dadara & Nagar Havelli\", \n",
    "                                \"Arunachal Pradesh\": \"Arunanchal Pradesh\", \n",
    "                                \"Delhi\": \"NCT of Delhi\"}, inplace=True)\n",
    "\n",
    "        # 3.removing rows for invalid states\n",
    "        state_list = ['Andaman & Nicobar Island', 'Andhra Pradesh', 'Arunanchal Pradesh', 'Assam', 'Bihar', 'Chandigarh', 'Chhattisgarh',\n",
    "                    'Dadara & Nagar Havelli', 'Daman & Diu', 'Goa', 'Gujarat', 'Haryana', 'Himachal Pradesh', 'Jammu & Kashmir',\n",
    "                    'Jharkhand', 'Karnataka', 'Kerala', 'Lakshadweep', 'Madhya Pradesh', 'Maharashtra', 'Manipur', 'Meghalaya', 'Mizoram',\n",
    "                    'NCT of Delhi', 'Nagaland', 'Odisha', 'Puducherry', 'Punjab', 'Rajasthan', 'Sikkim', 'Tamil Nadu', 'Telangana', 'Tripura',\n",
    "                    'Uttar Pradesh', 'Uttarakhand', 'West Bengal']\n",
    "        df_stepThree.drop(df_stepThree[~df_stepThree[\"state\"].isin(state_list)].index, inplace=True)\n",
    "\n",
    "\n",
    "        # --------------------------------------------------------    \n",
    "        #STEP FOUR: Appending date column and appending each day's day to the final DF\n",
    "        # 1.adding the date column\n",
    "        df_stepFour = df_stepThree.copy()\n",
    "        res = re.findall(\"(\\d+).jsonl\", hydratedJSONL)\n",
    "        date = (int)(res[0])\n",
    "        df_stepFour[\"date\"] = date\n",
    "        \n",
    "        # 2.appending individual DFs\n",
    "        final_df = final_df.append(df_stepFour, ignore_index = True)\n",
    "\n",
    "    #----------------------------------------------------------------\n",
    "    #exporting DF to CSV\n",
    "    final_df.to_csv(CSVpath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline(JSON_willDo, CSVpath)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d65ed39dd581ebcf9df35782933087a7829422d005b2b32c58247ec95bc3be21"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
